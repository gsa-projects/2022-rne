{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 설치, 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk\n",
    "\n",
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2096 804\n"
     ]
    }
   ],
   "source": [
    "csv.field_size_limit(100000000)\n",
    "\n",
    "def csv_to_json(csvfilename, jsonfilename, fieldnames):\n",
    "    csvfile = open(csvfilename, 'r')\n",
    "    jsonfile = open(jsonfilename, 'w')\n",
    "\n",
    "    reader = csv.DictReader(csvfile, fieldnames)\n",
    "\n",
    "    for row in reader:\n",
    "        json.dump(row, jsonfile)\n",
    "        jsonfile.write('\\n')\n",
    "\n",
    "csv_to_json('news_articles.csv', 'train_data.json', (\"author\",\"published\",\"raw_title\",\"raw_content\",\"language\",\"site_url\",\"main_img_url\",\"type\",\"label\",\"title\",\"content\",\"hasImage\"))\n",
    "DB = []\n",
    "with open('train_data.json') as j:\n",
    "    DB = list(map(json.loads, j))\n",
    "\n",
    "csv_to_json('test_data.csv', 'test_data.json', (\"unit_id\",\"title\",\"content\",\"source\",\"date\",\"location\",\"is_real\"))\n",
    "T = []\n",
    "with open('test_data.json') as j:\n",
    "    T = list(map(json.loads, j))\n",
    "\n",
    "print(len(DB), len(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 정제하기\n",
    "\n",
    "- 데이터 값 전처리 (정규식으로 영어, 특수기호 없애기)\n",
    "- 데이터 중 빈 값 제거\n",
    "- 데이터 중복 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = list(filter(lambda x: (x[\"content\"] != '' and x[\"content\"] != None), DB))\n",
    "T = list(filter(lambda x: (x[\"content\"] != '' and x[\"content\"] != None), T))\n",
    "\n",
    "# print(DB[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in DB:\n",
    "    v[\"content\"] = re.sub(r'[^a-zA-Z0-9 ]', '', v[\"content\"].lower())\n",
    "    v[\"content\"] = v[\"content\"].strip()\n",
    "\n",
    "for v in T:\n",
    "    v[\"content\"] = re.sub(r'[^a-zA-Z0-9 ]', '', v[\"content\"].lower())\n",
    "    v[\"content\"] = v[\"content\"].strip()\n",
    "\n",
    "# print(DB[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = list(filter(lambda x : x[\"content\"] != '' and x[\"content\"] != None, DB))\n",
    "T = list(filter(lambda x : x[\"content\"] != '' and x[\"content\"] != None, T))\n",
    "\n",
    "# print(DB[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = [dict(t) for t in {tuple(d.items()) for d in DB}]\n",
    "T = [dict(t) for t in {tuple(d.items()) for d in T}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_is = []\n",
    "for v in DB:\n",
    "    DB_is.append(1 if v['label'] == \"Real\" else 0)\n",
    "\n",
    "T_is = []\n",
    "for v in T:\n",
    "    T_is.append(int(v['is_real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0] [1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(DB_is[:5], T_is[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "DB_content = []\n",
    "for v in DB:\n",
    "    DB_content.append([w for w in v['content'].split() if w not in stopwords.words(\"english\")])\n",
    "\n",
    "T_content = []\n",
    "for v in T:\n",
    "    T_content.append([w for w in v['content'].split() if w not in stopwords.words(\"english\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['muslims', 'terrorize', 'hindus', 'wednesday', 'november', 'daniel', 'greenfield', 'official', 'media', 'narrative', 'muslims', 'worlds', 'greatest', 'victims', 'truth', 'especially', 'majority', 'muslim', 'countries', 'rather', 'strikingly', 'different', 'crowds', 'muslims', 'attacked', 'hindu', 'homes', 'temples', 'eastern', 'bangladesh', 'week', 'raising', 'concerns', 'authorities', 'taking', 'steps', 'curb', 'rising', 'religious', 'tensions', 'attacks', 'hindus', 'unusual', 'bangladesh', 'rare', 'see', 'multiple', 'crowds', 'targeting', 'temples', 'organized', 'way', 'sunday', 'monday', 'note', 'casual', 'language', 'times', 'muslim', 'religious', 'violence', 'commonplace', 'bangladesh', 'common', 'happening', 'scale', 'could', 'otherwise', 'islam', 'structurally', 'xenophobic', 'violently', 'bigoted', 'racist', 'origins', 'islams', 'faith', 'expressed', 'violent', 'campaign', 'nonmuslims', 'jihad', 'muslims', 'attacking', 'hindus', 'christians', 'jews', 'yazidis', 'group', 'means', 'must', 'wednesday', 'day', 'week', 'sunday', 'hundreds', 'muslims', 'entered', 'hindu', 'neighborhood', 'ransacked', 'temples', 'homes', 'families', 'mr', 'deb', 'said', 'said', 'mob', 'used', 'long', 'hard', 'sticks', 'locally', 'made', 'sharp', 'weapons', 'assault', 'hindus', 'found', 'least', 'people', 'including', 'priest', 'wounded', 'islam', 'purest', 'truest', 'form']\n",
      "=================\n",
      "['20', 'dead', 'syria', 'barrel', 'bomb', 'attack', 'aleppo', 'ngo', 'afp', 'monday', '16', 'jun', '2014', 'print', 'tweet', 'syrian', 'regime', 'helicopters', 'monday', 'dropped', 'barrel', 'bombs', 'oppositionheld', 'district', 'northern', 'aleppo', 'city', 'killing', 'least', '20', 'people', 'including', 'several', 'children', 'ngo', 'said', 'syrian', 'observatory', 'human', 'rights', 'said', '20', 'people', 'killed', 'others', 'injured', 'including', 'serious', 'condition', 'attacks', 'sukkari', 'neighbourhood']\n"
     ]
    }
   ],
   "source": [
    "print(f'{DB_content[0]}\\n=================\\n{T_content[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "희귀 단어 등장 비율: 6.050867768882943\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(DB_content)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tok.word_index)\n",
    "rare_cnt = 0\n",
    "total_freq = 0\n",
    "rare_freq = 0\n",
    "\n",
    "for key, value in tok.word_counts.items():\n",
    "    total_freq += value\n",
    "\n",
    "    if value < threshold:\n",
    "        rare_cnt += 1\n",
    "        rare_freq += value\n",
    "\n",
    "print(f'희귀 단어 등장 비율: {rare_freq / total_freq * 100}')\n",
    "\n",
    "size = total_cnt - rare_cnt + 1\n",
    "\n",
    "tok = Tokenizer(size)\n",
    "tok.fit_on_texts(DB_content)\n",
    "DB_content = tok.texts_to_sequences(DB_content)\n",
    "T_content = tok.texts_to_sequences(T_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indexes = [i for i in range(len(DB_content)) if len(DB_content[i]) == 0]\n",
    "\n",
    "for i in drop_indexes:\n",
    "    DB_content.pop(i)\n",
    "    DB_is.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_len = max(len(content) for content in DB_content)\n",
    "\n",
    "DB_content = pad_sequences(DB_content, maxlen=pad_len)\n",
    "T_content = pad_sequences(T_content, maxlen=pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   4, 337, 204],\n",
       "       [  0,   0,   0, ...,  71,   3, 148]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_content[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_is[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  983,  285,    9],\n",
       "       [   0,    0,    0, ..., 3767,  626,    6]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_content[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_is[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.6367 - acc: 0.6423 \n",
      "Epoch 1: val_acc improved from -inf to 0.65602, saving model to best_model.h5\n",
      "26/26 [==============================] - 744s 29s/step - loss: 0.6367 - acc: 0.6423 - val_loss: 0.6044 - val_acc: 0.6560\n",
      "Epoch 2/15\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.4440 - acc: 0.8359 \n",
      "Epoch 2: val_acc improved from 0.65602 to 0.67568, saving model to best_model.h5\n",
      "26/26 [==============================] - 783s 30s/step - loss: 0.4440 - acc: 0.8359 - val_loss: 0.5701 - val_acc: 0.6757\n",
      "Epoch 3/15\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.2735 - acc: 0.9219 \n",
      "Epoch 3: val_acc improved from 0.67568 to 0.69779, saving model to best_model.h5\n",
      "26/26 [==============================] - 802s 31s/step - loss: 0.2735 - acc: 0.9219 - val_loss: 0.5838 - val_acc: 0.6978\n",
      "Epoch 4/15\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.1141 - acc: 0.9699 \n",
      "Epoch 4: val_acc did not improve from 0.69779\n",
      "26/26 [==============================] - 826s 32s/step - loss: 0.1141 - acc: 0.9699 - val_loss: 0.7122 - val_acc: 0.6978\n",
      "Epoch 5/15\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.0757 - acc: 0.9785 \n",
      "Epoch 5: val_acc did not improve from 0.69779\n",
      "26/26 [==============================] - 719s 27s/step - loss: 0.0757 - acc: 0.9785 - val_loss: 0.8872 - val_acc: 0.6855\n",
      "Epoch 6/15\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.0577 - acc: 0.9779 \n",
      "Epoch 6: val_acc improved from 0.69779 to 0.70762, saving model to best_model.h5\n",
      "26/26 [==============================] - 709s 27s/step - loss: 0.0577 - acc: 0.9779 - val_loss: 0.8761 - val_acc: 0.7076\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "DB_is = np.array(DB_is)\n",
    "T_is = np.array(T_is)\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(size, embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(DB_content, DB_is, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 35s 1s/step - loss: 1.3080 - acc: 0.5125\n",
      "accuracy: 0.5124688148498535\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(f'accuracy: {loaded_model.evaluate(T_content, T_is)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(string):\n",
    "    string = re.sub(r'[^a-zA-Z0-9 ]', '', string.lower())\n",
    "    string = [w for w in string.split() if w not in stopwords.words(\"english\")]\n",
    "    tknized = tok.texts_to_sequences([string])\n",
    "    pad_new = pad_sequences(tknized, maxlen = size)\n",
    "    score = float(loaded_model.predict(pad_new))\n",
    "    print(f'{score} positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "0.10758146643638611 positive\n"
     ]
    }
   ],
   "source": [
    "predict(\"is this fake news?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "0.3122520446777344 positive\n"
     ]
    }
   ],
   "source": [
    "predict(\"Trump got his stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "0.3235936462879181 positive\n"
     ]
    }
   ],
   "source": [
    "predict('how can i get high score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
