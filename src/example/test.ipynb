{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def get_data(file_url, file_name):\n",
    "    urllib.request.urlretrieve(file_url, filename=file_name)\n",
    "\n",
    "    return pd.read_table(file_name)\n",
    "\n",
    "def text_processing(train_data):\n",
    "    processed_data = train_data\n",
    "    processed_data.drop_duplicates(subset=['document'], inplace=True)   # document 열의 중복 제거 (원본 값 변경 허용)\n",
    "    processed_data = processed_data.dropna(how = 'any') # document 열의 NaN 제거\n",
    "    processed_data['document'] = processed_data['document'].str.replace(\"[^ㄱ-힣 ]\", \"\")    # document 열의 값들 한국어와 공백만 남기고 모두 제거\n",
    "    processed_data['document'].replace(\"\", np.nan, inplace=True)    # 빈칸 document값은 NaN으로 변환 (원본 값 변경 허용)\n",
    "    processed_data = processed_data.dropna(how = 'any') # document 열의 NaN 제거\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def tokenization(data):\n",
    "    okt = Okt()\n",
    "    tokenized = list()\n",
    "\n",
    "    for sentence in tqdm(data['document']):\n",
    "        tokenized.append([pos_set[0] for pos_set in okt.pos(okt.normalize(sentence)) if pos_set[1] != 'Josa'])\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "def integer_encoding(tokenized_data, processed_data, min_freq=3):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(tokenized_data)\n",
    "\n",
    "    word_count = len(tokenizer.word_index) + 1\n",
    "    for (_, freq) in tokenizer.word_counts.items():\n",
    "        if freq < min_freq: word_count -= 1\n",
    "\n",
    "    tokenizer = Tokenizer(word_count)\n",
    "    tokenizer.fit_on_texts(tokenized_data)\n",
    "\n",
    "    ret = tokenizer.texts_to_sequences(tokenized_data)\n",
    "    sol = np.array(processed_data['label'])\n",
    "    drop = [idx for (idx, sentence) in enumerate(ret) if len(sentence) == 0]\n",
    "\n",
    "    return {\n",
    "        'encoded': np.delete(ret, drop, axis=0),\n",
    "        'solution': np.delete(sol, drop, axis=0),\n",
    "        'size': word_count\n",
    "    }\n",
    "\n",
    "def padding(integer_encoded, max_len=None):\n",
    "    return pad_sequences(integer_encoded) if max_len is None else pad_sequences(integer_encoding, max_len=max_len)\n",
    "\n",
    "def train(train_data, solution, size, show_accuracy=True, best_model_name='best_model', embedding_dim=100, hidden_units=128, verbose=1, patience=4, epochs=15, batch_size=64, validation_split=0.2, save_best_only=True):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(size, embedding_dim))\n",
    "    model.add(LSTM(hidden_units))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=verbose, patience=patience)\n",
    "    mc = ModelCheckpoint(best_model_name + '.h5', monitor='val_acc', mode='max', verbose=verbose, save_best_only=save_best_only)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model.fit(train_data, solution, epochs=epochs, callbacks=[es, mc], batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    if show_accuracy: print(\"accuracy: %.4f\" % (load_model(best_model_name + '.h5').evaluate(train_data, solution)[1]))\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(review, word_count, pad_max_len=None, best_model_name='best_model'):\n",
    "    okt = Okt()\n",
    "    tokenizer = Tokenizer(word_count)\n",
    "\n",
    "    review = review.replace(\"[^ㄱ-힣 ]\", \"\")\n",
    "    review = [[pos_set[0] for pos_set in okt.pos(okt.normalize(review)) if pos_set[1] != 'Josa']]\n",
    "    encoded = tokenizer.texts_to_sequences(review)\n",
    "    pad_new = padding(encoded, max_len=pad_max_len)\n",
    "    score = float(load_model(best_model_name + '.h5').predict(pad_new)) # 예측\n",
    "    \n",
    "    return score\n",
    "\n",
    "class AI:\n",
    "    def __init__(self, train_file_url, train_file_name):\n",
    "        self.data = text_processing(get_data(train_file_url, train_file_name))\n",
    "        self.tokenized = tokenization(self.data)\n",
    "        self.encoded = integer_encoding(self.tokenized, self.data)\n",
    "        self.padded = padding(self.encoded['encoded'])\n",
    "        self.model = train(self.padded, self.encoded['solution'], self.encoded['size'])\n",
    "\n",
    "    def is_positive(self, review, is_print=True):\n",
    "        score = predict(review, self.encoded['size'])\n",
    "        if is_print: print(\"{:.2f}% positive\" % (score * 100))\n",
    "        else: return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_16380\\3553975117.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  processed_data['document'] = processed_data['document'].str.replace(\"[^ㄱ-힣 ]\", \"\")    # document 열의 값들 한국어와 공백만 남기고 모두 제거\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_16380\\3553975117.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_data['document'] = processed_data['document'].str.replace(\"[^ㄱ-힣 ]\", \"\")    # document 열의 값들 한국어와 공백만 남기고 모두 제거\n",
      "C:\\Users\\SAMSUNG\\AppData\\Local\\Temp\\ipykernel_16380\\3553975117.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_data['document'].replace(\"\", np.nan, inplace=True)    # 빈칸 document값은 NaN으로 변환 (원본 값 변경 허용)\n",
      "100%|██████████| 145804/145804 [09:30<00:00, 255.68it/s]\n",
      "c:\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:5030: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1813/1813 [==============================] - ETA: 0s - loss: 0.3942 - acc: 0.8192\n",
      "Epoch 1: val_acc improved from -inf to 0.84346, saving model to best_model.h5\n",
      "1813/1813 [==============================] - 170s 92ms/step - loss: 0.3942 - acc: 0.8192 - val_loss: 0.3648 - val_acc: 0.8435\n",
      "Epoch 2/15\n",
      "1812/1813 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8617\n",
      "Epoch 2: val_acc improved from 0.84346 to 0.85536, saving model to best_model.h5\n",
      "1813/1813 [==============================] - 170s 94ms/step - loss: 0.3226 - acc: 0.8617 - val_loss: 0.3361 - val_acc: 0.8554\n",
      "Epoch 3/15\n",
      "1813/1813 [==============================] - ETA: 0s - loss: 0.2913 - acc: 0.8789\n",
      "Epoch 3: val_acc improved from 0.85536 to 0.86184, saving model to best_model.h5\n",
      "1813/1813 [==============================] - 221s 122ms/step - loss: 0.2913 - acc: 0.8789 - val_loss: 0.3256 - val_acc: 0.8618\n",
      "Epoch 4/15\n",
      "1813/1813 [==============================] - ETA: 0s - loss: 0.2689 - acc: 0.8909\n",
      "Epoch 4: val_acc did not improve from 0.86184\n",
      "1813/1813 [==============================] - 269s 148ms/step - loss: 0.2689 - acc: 0.8909 - val_loss: 0.3202 - val_acc: 0.8618\n",
      "Epoch 5/15\n",
      "1813/1813 [==============================] - ETA: 0s - loss: 0.2506 - acc: 0.8991\n",
      "Epoch 5: val_acc did not improve from 0.86184\n",
      "1813/1813 [==============================] - 291s 161ms/step - loss: 0.2506 - acc: 0.8991 - val_loss: 0.3260 - val_acc: 0.8612\n",
      "Epoch 6/15\n",
      "1813/1813 [==============================] - ETA: 0s - loss: 0.2348 - acc: 0.9070\n",
      "Epoch 6: val_acc did not improve from 0.86184\n",
      "1813/1813 [==============================] - 273s 151ms/step - loss: 0.2348 - acc: 0.9070 - val_loss: 0.3541 - val_acc: 0.8494\n",
      "Epoch 7/15\n",
      "1813/1813 [==============================] - ETA: 0s - loss: 0.2189 - acc: 0.9133\n",
      "Epoch 7: val_acc did not improve from 0.86184\n",
      "1813/1813 [==============================] - 292s 161ms/step - loss: 0.2189 - acc: 0.9133 - val_loss: 0.3430 - val_acc: 0.8564\n",
      "Epoch 8/15\n",
      "1813/1813 [==============================] - ETA: 0s - loss: 0.2030 - acc: 0.9205\n",
      "Epoch 8: val_acc did not improve from 0.86184\n",
      "1813/1813 [==============================] - 304s 168ms/step - loss: 0.2030 - acc: 0.9205 - val_loss: 0.3619 - val_acc: 0.8543\n",
      "Epoch 8: early stopping\n",
      "4531/4531 [==============================] - 153s 33ms/step - loss: 0.2750 - acc: 0.8868\n",
      "accuracy: 0.8868\n"
     ]
    }
   ],
   "source": [
    "ai = AI(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", \"ratings_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Python310\\lib\\site-packages\\keras\\backend.py\", line 4950, in <listcomp>\n        inputs, [inp[0] for inp in flatted_inputs]\n\n    ValueError: Exception encountered when calling layer \"lstm\" \"                 f\"(type LSTM).\n    \n    slice index 0 of dimension 0 out of bounds. for '{{node strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](transpose, strided_slice_1/stack, strided_slice_1/stack_1, strided_slice_1/stack_2)' with input shapes: [0,?,100], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n    \n    Call arguments received by layer \"lstm\" \"                 f\"(type LSTM):\n      • inputs=tf.Tensor(shape=(None, 0, 100), dtype=float32)\n      • mask=None\n      • training=False\n      • initial_state=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SAMSUNG\\OneDrive\\문서\\GitHub\\gsa_stuff\\src\\R&E\\LSTM\\test.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SAMSUNG/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/gsa_stuff/src/R%26E/LSTM/test.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ai\u001b[39m.\u001b[39;49mis_positive(\u001b[39m\"\u001b[39;49m\u001b[39m올해 최고의 영화! 세 번 넘게 봐도 질리지가 않네요.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\SAMSUNG\\OneDrive\\문서\\GitHub\\gsa_stuff\\src\\R&E\\LSTM\\test.ipynb 셀 3\u001b[0m in \u001b[0;36mAI.is_positive\u001b[1;34m(self, review, is_print)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SAMSUNG/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/gsa_stuff/src/R%26E/LSTM/test.ipynb#W2sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_positive\u001b[39m(\u001b[39mself\u001b[39m, review, is_print\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SAMSUNG/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/gsa_stuff/src/R%26E/LSTM/test.ipynb#W2sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     score \u001b[39m=\u001b[39m predict(review, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoded[\u001b[39m'\u001b[39;49m\u001b[39msize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SAMSUNG/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/gsa_stuff/src/R%26E/LSTM/test.ipynb#W2sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     \u001b[39mif\u001b[39;00m is_print: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m positive\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (score \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/SAMSUNG/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/gsa_stuff/src/R%26E/LSTM/test.ipynb#W2sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     \u001b[39melse\u001b[39;00m: \u001b[39mreturn\u001b[39;00m score\n",
      "\u001b[1;32mc:\\Users\\SAMSUNG\\OneDrive\\문서\\GitHub\\gsa_stuff\\src\\R&E\\LSTM\\test.ipynb 셀 3\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(review, word_count, pad_max_len, best_model_name)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SAMSUNG/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/gsa_stuff/src/R%26E/LSTM/test.ipynb#W2sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m encoded \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(review)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SAMSUNG/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/gsa_stuff/src/R%26E/LSTM/test.ipynb#W2sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m pad_new \u001b[39m=\u001b[39m padding(encoded, max_len\u001b[39m=\u001b[39mpad_max_len)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SAMSUNG/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/gsa_stuff/src/R%26E/LSTM/test.ipynb#W2sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(load_model(best_model_name \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mpredict(pad_new)) \u001b[39m# 예측\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SAMSUNG/OneDrive/%EB%AC%B8%EC%84%9C/GitHub/gsa_stuff/src/R%26E/LSTM/test.ipynb#W2sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filehn3n_1lb.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Python310\\lib\\site-packages\\keras\\backend.py\", line 4950, in <listcomp>\n        inputs, [inp[0] for inp in flatted_inputs]\n\n    ValueError: Exception encountered when calling layer \"lstm\" \"                 f\"(type LSTM).\n    \n    slice index 0 of dimension 0 out of bounds. for '{{node strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](transpose, strided_slice_1/stack, strided_slice_1/stack_1, strided_slice_1/stack_2)' with input shapes: [0,?,100], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\n    \n    Call arguments received by layer \"lstm\" \"                 f\"(type LSTM):\n      • inputs=tf.Tensor(shape=(None, 0, 100), dtype=float32)\n      • mask=None\n      • training=False\n      • initial_state=None\n"
     ]
    }
   ],
   "source": [
    "ai.is_positive(\"올해 최고의 영화! 세 번 넘게 봐도 질리지가 않네요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
